{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models.detection as models\n",
    "import torchvision.transforms.functional as transforms\n",
    "\n",
    "import torch\n",
    "print(\"Torch version:\", torch.__version__) # 0.13.1+cu116\n",
    "\n",
    "import objectdetectiontools\n",
    "print(\"Object Detection Tools version:\", objectdetectiontools.__version__) # 1.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input parameters\n",
    "In order to run this notebook the following parameters need to be provided:\n",
    "- `model_path` the path to the saved model to use. The notebook `flowerpower.ipynb` can be used to create models.\n",
    "- `image_path` the path to the image to run object detection on.\n",
    "- `output_dir` the destination directory for the output images. Will be created if it does not exist yet.\n",
    "- `threshold` the minimum accuracy for a detection to be drawn as a bounding box in the output file.\n",
    "- `font_file` the filename of the font to use for the labels in the output file. Will look in the default OS font directory.\n",
    "- `font-size` the size of the font to use for the labels in the output file.\n",
    "- `line-width` the width of the line used to draw bounding boxes in the output file.\n",
    "- `color` the color of the line used to draw bounding boxes, as well as for the font used for the labels in the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"output/frcnn\"\n",
    "image_path = \"C:/Users/Bas/Desktop/a.jpg\"\n",
    "output_dir = \"C:/Users/Bas/Desktop/Inference\"\n",
    "\n",
    "threshold = .6\n",
    "\n",
    "font_file = \"arial.ttf\"\n",
    "font_size = 18\n",
    "line_width = 3\n",
    "color = \"#FF00DC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_path == \"\" or image_path == \"\" or output_dir == \"\":\n",
    "    raise Exception(\"None of the path parameters can be empty, please provide a value for all of them\")\n",
    "if threshold > 1 or threshold < .01:\n",
    "    raise Exception(\"Parameter 'threshold' cannot be larger than 1 or smaller than .01\")\n",
    "if font_file == \"\":\n",
    "    raise Exception(\"Parameter 'font_file' cannot be empty\")\n",
    "if font_size < 1:\n",
    "    raise Exception(\"Parameter 'font_size' cannot be smaller than 1\")\n",
    "if line_width < 1:\n",
    "    raise Exception(\"Parameter 'line_width' cannot be smaller than 1\")\n",
    "if color == \"\":\n",
    "    raise Exception(\"Parameter 'color' cannot be empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“¥ Loading the model\n",
    "Loading the model from a previously saved state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(model_path, \"classes.json\"))\n",
    "classes = json.load(f)\n",
    "f.close()\n",
    "model = models.fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "model.roi_heads.box_predictor = models.faster_rcnn.FastRCNNPredictor(model.roi_heads.box_predictor.cls_score.in_features, len(classes))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state = torch.load(f=os.path.join(model_path, \"model\"), map_location=device)\n",
    "model.load_state_dict(state_dict=state)\n",
    "model.eval()\n",
    "print(\"Loaded a model of type\", str(type(model).__name__), \"for\", len(classes), \"classes on\", device)\n",
    "print(\"The following classes are known:\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ´ Preprocessing\n",
    "Cut the input image into tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles, rows, columns = objectdetectiontools.images.split_single(image_path, model.transform.max_size)\n",
    "fig, subplots = plt.subplots(nrows=rows, ncols=columns, figsize=(5*columns, 5*rows))\n",
    "subplots = subplots.flatten()\n",
    "for i, t in enumerate(tiles):\n",
    "    subplots[i].set_xticks([])\n",
    "    subplots[i].set_yticks([])\n",
    "    subplots[i].imshow(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”Ž Inference\n",
    "Run a prediction on every tile from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, image, threshold):\n",
    "    tensor = transforms.pil_to_tensor(image).unsqueeze(dim=0) / 255\n",
    "    predictions = model(tensor)\n",
    "    predictions[0][\"boxes\"] = predictions[0][\"boxes\"][predictions[0][\"scores\"] > threshold]\n",
    "    predictions[0][\"labels\"] = predictions[0][\"labels\"][predictions[0][\"scores\"] > threshold]\n",
    "    predictions[0][\"scores\"] = predictions[0][\"scores\"][predictions[0][\"scores\"] > threshold]\n",
    "    return predictions\n",
    "\n",
    "def add_bounding_boxes(image, classes, predictions, font_size, line_width, color):\n",
    "    from torchvision.utils import draw_bounding_boxes\n",
    "    annotations = []\n",
    "    for p in predictions[0][\"labels\"].detach().numpy():\n",
    "        annotations.append({\"name\": classes[p]})\n",
    "    labels = [\"{} {}%\".format(label[\"name\"], int(prob *100)) for label, prob in zip(annotations, predictions[0][\"scores\"].detach().numpy())]\n",
    "    output = draw_bounding_boxes(image=transforms.pil_to_tensor(image).unsqueeze(dim=0)[0], boxes=predictions[0][\"boxes\"], labels=labels, colors=color, width=line_width, font=font_file, font_size=font_size)\n",
    "    return transforms.to_pil_image(output) \n",
    "\n",
    "print(\"Running inference for\", len(tiles), \"tiles:\")\n",
    "fig, subplots = plt.subplots(nrows=rows, ncols=columns, figsize=(5*columns, 5*rows))\n",
    "subplots = subplots.flatten()\n",
    "result_images = []\n",
    "result_predictions = []\n",
    "for index, tile in enumerate(tiles):\n",
    "    print(\"Tile\", index+1)\n",
    "    predictions = inference(model, tile, threshold)\n",
    "    result_predictions.append(predictions)\n",
    "    result_image = add_bounding_boxes(tile, classes, predictions, font_size, line_width, color) if len(predictions[0][\"boxes\"]) > 0 else tile\n",
    "    result_images.append(result_image)\n",
    "    subplots[index].set_xticks([])\n",
    "    subplots[index].set_yticks([])\n",
    "    subplots[index].imshow(result_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸª¡ Stitch\n",
    "Stitch the tiles back together again and save the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "output_file = os.path.join(output_dir, os.path.basename(image_path))\n",
    "result = objectdetectiontools.images.stitch(result_images, columns)\n",
    "result.save(output_file)\n",
    "print(\"Inference image saved at\", output_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§® Counting\n",
    "The code below generates an overview of the found labels and the number of times they appear on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countings = dict()\n",
    "for prediction in result_predictions:\n",
    "    for label in prediction[0][\"labels\"].detach().numpy():\n",
    "        label = classes[label]\n",
    "        countings[label] = countings[label] +1 if label in countings else 1\n",
    "\n",
    "for key, value in countings.items():\n",
    "    print(key, \":\", value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
